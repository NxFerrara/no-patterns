{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open3d import io\n",
    "from open3d import geometry\n",
    "from open3d import utility\n",
    "from open3d import visualization\n",
    "import meshcut\n",
    "import scipy.interpolate as si\n",
    "import splipy as sp\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import networkx as nx\n",
    "import glob\n",
    "import json\n",
    "import ast\n",
    "import copy\n",
    "from geomdl import BSpline\n",
    "from geomdl import multi\n",
    "from geomdl import knotvector\n",
    "from geomdl import operations\n",
    "from geomdl import fitting\n",
    "from geomdl.visualization import VisMPL\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pcd(points):\n",
    "    pcd = geometry.PointCloud()\n",
    "    pcd.points = utility.Vector3dVector(points)\n",
    "    return pcd\n",
    "\n",
    "def extract_landmarks(meshdir, landmarkscandir, landmarkdir, pomdir, percent_scan=0.25):\n",
    "    mesh = io.read_triangle_mesh(meshdir)\n",
    "    landmarkscans = glob.glob(f'{landmarkscandir}*.ply')\n",
    "    mesh_vertices = np.asarray(mesh.vertices)\n",
    "    pcd = create_pcd(mesh_vertices)\n",
    "    tree = geometry.KDTreeFlann(pcd)\n",
    "    landmarkdata = {}\n",
    "    for filedir in landmarkscans:\n",
    "        landmark_mesh = io.read_triangle_mesh(filedir)\n",
    "        vertices = np.asarray(landmark_mesh.vertices)\n",
    "        unordered_idx = []\n",
    "        for vertex in vertices:\n",
    "            [_, idx, d] = tree.search_knn_vector_3d(vertex, 1)\n",
    "            unordered_idx.append([idx[0], d[0]])\n",
    "        ordered_idx = np.asarray(sorted(unordered_idx, key=lambda x: x[1]))[:,0].astype(np.int32)\n",
    "        percent_idx = ordered_idx[:int(len(ordered_idx)*percent_scan)+1]\n",
    "        percent_points = mesh_vertices[percent_idx]\n",
    "        name = filedir.split('_')[-1][:-4]\n",
    "        landmark_mean = percent_points.mean(axis=0)\n",
    "        [_, idx, _] = tree.search_knn_vector_3d(landmark_mean, 1)\n",
    "        landmark_position = mesh_vertices[idx[0]]\n",
    "        landmarkdata[name] = {\n",
    "            'position': tuple(landmark_position)\n",
    "        }\n",
    "    with open(landmarkdir, 'w') as file:\n",
    "        json.dump(landmarkdata, file, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectdir = '/Users/nickf/Dropbox/Nicky T-Shirt Project/Subject Scans/Male Scans/Nicholas Ferrara/'\n",
    "meshdir = f'{subjectdir}NF_SCAN_V3/NF_REGULAR_V1.4.1_M2_ADJ.ply'\n",
    "landmarkscandir = f'{subjectdir}NF_LANDMARKS/'\n",
    "landmarkdir = f'{subjectdir}NF_LANDMARKS.json'\n",
    "pomdir = f'{subjectdir}NF_POMS_V2.json'\n",
    "landmarkdata = extract_landmarks(meshdir, landmarkscandir, landmarkdir, pomdir, percent_scan=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_tolerance(mesh, num_std):\n",
    "    triangles = np.asarray(mesh.triangles)\n",
    "    vertices = np.asarray(mesh.vertices)\n",
    "    max_distances = []\n",
    "    ordered_triangles = []\n",
    "    for triangle in triangles:\n",
    "        triangle_vertices = [vertices[vertex_idx] for vertex_idx in triangle]\n",
    "        centroid = np.asarray(triangle_vertices).mean(axis=0)\n",
    "        centroid_distances = []\n",
    "        for vertex in triangle_vertices:\n",
    "            diff_vector = centroid - vertex\n",
    "            diff_length = np.sqrt(np.dot(diff_vector, diff_vector))\n",
    "            centroid_distances.append(diff_length)\n",
    "        max_centroid_distance = np.asarray(centroid_distances).max()\n",
    "        max_distances.append(max_centroid_distance)\n",
    "        ordered_triangles.append([centroid, triangle])\n",
    "    tolerance = np.mean(max_distances) + num_std*np.std(max_distances)\n",
    "    return ordered_triangles, tolerance\n",
    "\n",
    "def read_scan(meshdir, landmarksdir, pomsdir):\n",
    "    mesh = io.read_triangle_mesh(meshdir)\n",
    "    mesh.compute_vertex_normals()\n",
    "    \n",
    "    with open(landmarksdir, 'r') as readlandmarks:\n",
    "        landmarks = json.load(readlandmarks)\n",
    "        with open(pomsdir, 'r') as readpoms:\n",
    "            poms = json.load(readpoms)\n",
    "            pom_landmarks = {}\n",
    "            for key in poms:\n",
    "                landmark_points = []\n",
    "                for temp_key in poms[key]['landmarks']:\n",
    "                    landmark_points.append(landmarks[temp_key]['position'])\n",
    "                fitlandmark_points = []\n",
    "                for temp_key in poms[key]['fitlandmarks']:\n",
    "                    fitlandmark_points.append(landmarks[temp_key]['position'])\n",
    "                landmark_constraints = []\n",
    "                for temp_key in poms[key]['landmarks']:\n",
    "                    landmark_constraints.append(landmarks[temp_key]['constraints'])\n",
    "                landmark_dict = {\n",
    "                    'fitlandmark_points': np.asarray(fitlandmark_points),\n",
    "                    'landmark_points': np.asarray(landmark_points),\n",
    "                    'periodic': poms[key]['periodic'],\n",
    "                    'constraints': landmark_constraints,\n",
    "                    'pom_alignment': [],\n",
    "                    'section_normals': [],\n",
    "                    'pom_points': None,\n",
    "                    'pom_normals': None\n",
    "                }\n",
    "                pom_landmarks[key] = landmark_dict\n",
    "    return mesh, pom_landmarks\n",
    "\n",
    "def transform_wireframe(pom_landmarks_temp, key, delta):\n",
    "    pom_landmarks = copy.deepcopy(pom_landmarks_temp)\n",
    "    landmark_points = pom_landmarks[key]['landmark_points']\n",
    "    landmark_constraints = pom_landmarks[key]['constraints']\n",
    "    pom_points = pom_landmarks[key]['pom_points']\n",
    "    pom_normals = pom_landmarks[key]['pom_normals']\n",
    "    for landmark_idx, landmark_pt in enumerate(landmark_points):\n",
    "        current_constraint = False\n",
    "        next_constraint = False\n",
    "        next_landmark_idx = landmark_idx + 1\n",
    "        if pom_landmarks[key]['periodic']:\n",
    "            if next_landmark_idx == len(landmark_points):\n",
    "                next_landmark_idx = 0\n",
    "                next_landmark_pom_idx = len(pom_points) - 1\n",
    "            else:\n",
    "                next_landmark_pom_idx = int(np.argwhere((pom_points == landmark_points[next_landmark_idx]).all(axis=1))[0])\n",
    "        elif not next_landmark_idx == len(landmark_points):\n",
    "            next_landmark_pom_idx = int(np.argwhere((pom_points == landmark_points[next_landmark_idx]).all(axis=1))[0])\n",
    "        else:\n",
    "            next_landmark_idx = 0\n",
    "            next_landmark_pom_idx = int(np.argwhere((pom_points == landmark_points[0]).all(axis=1))[0])\n",
    "        landmark_pom_idx = int(np.argwhere((pom_points == landmark_pt).all(axis=1))[0])\n",
    "        \n",
    "        landmark_normal = pom_normals[landmark_pom_idx]\n",
    "        if landmark_constraints[next_landmark_idx][0] == 1:\n",
    "            next_constraint = True\n",
    "            c = delta/(next_landmark_pom_idx - landmark_pom_idx)\n",
    "            current_transform = delta*landmark_normal\n",
    "        elif landmark_constraints[landmark_idx][0] == 1:\n",
    "            current_constraint = True\n",
    "            c = delta/(next_landmark_pom_idx - landmark_pom_idx)\n",
    "            current_transform = 0\n",
    "        else:\n",
    "            current_transform = delta*landmark_normal\n",
    "        landmark_transformed = landmark_pt + current_transform\n",
    "        pom_points[landmark_pom_idx] = landmark_transformed\n",
    "        landmark_points[landmark_idx] = landmark_transformed\n",
    "        current_pt_idx = landmark_pom_idx\n",
    "        if pom_landmarks[key]['periodic']:\n",
    "            while True:\n",
    "                next_pt_idx = current_pt_idx + 1\n",
    "                if landmark_idx == len(landmark_points) - 1 and next_pt_idx == len(pom_points):\n",
    "                    break\n",
    "                next_pt = pom_points[next_pt_idx]\n",
    "                if (landmark_points == next_pt).all(axis=1).any():\n",
    "                    break\n",
    "                next_pt_normal = pom_normals[next_pt_idx]\n",
    "                if current_constraint:\n",
    "                    next_pt_transform = c*(next_pt_idx - landmark_pom_idx)\n",
    "                elif next_constraint:\n",
    "                    next_pt_transform = c*(next_landmark_pom_idx - next_pt_idx)\n",
    "                else:\n",
    "                    next_pt_transform = np.dot(current_transform, next_pt_normal) / np.dot(next_pt_normal, next_pt_normal)\n",
    "                pom_points[next_pt_idx] = next_pt + next_pt_normal*next_pt_transform\n",
    "                current_pt_idx += 1\n",
    "                current_pt_normal = pom_normals[current_pt_idx]\n",
    "                current_transform = delta*current_pt_normal\n",
    "        elif landmark_idx < len(landmark_points) - 1:\n",
    "            while True:\n",
    "                next_pt_idx = current_pt_idx + 1\n",
    "                next_pt = pom_points[next_pt_idx]\n",
    "                if (landmark_points == next_pt).all(axis=1).any():\n",
    "                    break\n",
    "                next_pt_normal = pom_normals[next_pt_idx]\n",
    "                if current_constraint:\n",
    "                    next_pt_transform = c*(next_pt_idx - landmark_pom_idx)\n",
    "                elif next_constraint:\n",
    "                    next_pt_transform = c*(next_landmark_pom_idx - next_pt_idx)\n",
    "                else:\n",
    "                    next_pt_transform = np.dot(current_transform, next_pt_normal) / np.dot(next_pt_normal, next_pt_normal)\n",
    "                pom_points[next_pt_idx] = next_pt + next_pt_normal*next_pt_transform\n",
    "                current_pt_idx += 1\n",
    "                current_pt_normal = pom_normals[current_pt_idx]\n",
    "                current_transform = delta*current_pt_normal\n",
    "    if pom_landmarks[key]['periodic']:\n",
    "        landmark_idx = -1\n",
    "        landmark_pt = landmark_points[landmark_idx]\n",
    "        landmark_pom_idx = int(np.argwhere((pom_points == landmark_pt).all(axis=1))[0])\n",
    "        landmark_normal = pom_normals[landmark_pom_idx]\n",
    "        current_transform = delta*landmark_normal\n",
    "        landmark_transformed = landmark_pt + current_transform\n",
    "        pom_points[landmark_pom_idx] = landmark_transformed\n",
    "    transformed_pcd = geometry.PointCloud()\n",
    "    transformed_pcd.points = utility.Vector3dVector(pom_points)\n",
    "    transformed_pcd.normals = utility.Vector3dVector(pom_normals)\n",
    "    return transformed_pcd\n",
    "\n",
    "def create_wireframe(mesh, pom_landmarks, num_std=10, num_neighbors=5):\n",
    "    base_poms = []\n",
    "    transformed_poms = []\n",
    "    triangles = np.asarray(mesh.triangles)\n",
    "    vertices = np.asarray(mesh.vertices)\n",
    "    mesh_vertex_normals = np.asarray(mesh.vertex_normals)\n",
    "    ordered_triangles, centroid_tolerance = distance_tolerance(mesh, num_std)\n",
    "    f = FloatProgress(min=1, max=len(pom_landmarks.keys()))\n",
    "    display(f)\n",
    "    for key in pom_landmarks:\n",
    "        landmark_points = pom_landmarks[key]['landmark_points']\n",
    "        fitlandmark_points = pom_landmarks[key]['fitlandmark_points']\n",
    "        if len(fitlandmark_points) > 0:\n",
    "            fit_points = np.vstack((landmark_points, fitlandmark_points))\n",
    "            section_mean = fit_points.mean(axis=0)\n",
    "        else:\n",
    "            section_mean = landmark_points.mean(axis=0)\n",
    "        initial_landmark_idx = 0\n",
    "        if pom_landmarks[key]['periodic']:\n",
    "            initial_landmark_idx = -1\n",
    "        pom_points = None\n",
    "        for landmark_idx in range(initial_landmark_idx, len(landmark_points)-1):\n",
    "            current_landmark = landmark_points[landmark_idx]\n",
    "            next_landmark = landmark_points[landmark_idx+1]\n",
    "            diff_vector1 = current_landmark - section_mean\n",
    "            diff_vector2 = next_landmark - section_mean\n",
    "            section_normal = np.cross(diff_vector1, diff_vector2)\n",
    "            normal_length = np.sqrt(np.dot(section_normal, section_normal))\n",
    "            pom_landmarks[key]['section_normals'].append(section_normal)\n",
    "            # Project Triangle Centroid onto section plane to find the triangles within the\n",
    "            # tolerance of maximum distance from the plane\n",
    "            d = np.dot(section_normal, section_mean)\n",
    "            valid_triangles = []\n",
    "            for centroid, triangle in ordered_triangles:\n",
    "                if abs(np.dot(section_normal, centroid) - d) / normal_length <= centroid_tolerance:\n",
    "                    valid_triangles.append(triangle)\n",
    "            valid_triangles = np.asarray(valid_triangles)\n",
    "            # Slicing the mesh\n",
    "            mesh_slices = meshcut.cross_section(vertices, valid_triangles,\n",
    "                                                plane_orig=tuple(section_mean), plane_normal=tuple(section_normal))\n",
    "            # Determine the slice by the presents of the landmarks\n",
    "            current_slice = None\n",
    "            current_landmark_idx = None\n",
    "            next_landmark_idx = None\n",
    "            for slice_section in mesh_slices:\n",
    "                current_landmark_equality = (slice_section == current_landmark).all(axis=1)\n",
    "                next_landmark_equality = (slice_section == next_landmark).all(axis=1)\n",
    "                if current_landmark_equality.any() and next_landmark_equality.any():\n",
    "                    current_slice = slice_section\n",
    "                    current_landmark_idx = int(np.argwhere(current_landmark_equality))\n",
    "                    next_landmark_idx = int(np.argwhere(next_landmark_equality)) \n",
    "            current_slice = np.vstack((current_slice[current_landmark_idx:], current_slice[:current_landmark_idx]))\n",
    "            next_landmark_idx -= current_landmark_idx\n",
    "            current_landmark_idx = 0\n",
    "            if next_landmark_idx < 0:\n",
    "                next_landmark_idx += len(current_slice)\n",
    "            if next_landmark_idx < len(current_slice) - next_landmark_idx:\n",
    "                current_slice = current_slice[:next_landmark_idx+1]\n",
    "            else:\n",
    "                current_slice = np.vstack((current_slice[next_landmark_idx:], current_slice[current_landmark_idx]))\n",
    "                current_slice = np.flip(current_slice, axis=0)\n",
    "            if type(pom_points) == type(None):\n",
    "                pom_points = current_slice\n",
    "            else:\n",
    "                assert (pom_points[-1] == current_slice[0]).all(), f'{pom_points, current_slice}\\nPrevious Section not connected by landmark'\n",
    "                pom_points = np.vstack((pom_points[:-1], current_slice))\n",
    "        # Storing vertex normals of the vertices within the valid triangles for normal estimation        \n",
    "        valid_vertices = []\n",
    "        for triangle in valid_triangles:\n",
    "            for vertex_idx in triangle:\n",
    "                valid_vertices.append(vertex_idx)\n",
    "        vertices_idx = np.unique(np.asarray(valid_vertices))\n",
    "        valid_vertices = np.asarray([vertices[vertex_idx] for vertex_idx in vertices_idx])\n",
    "        valid_vertex_normals = np.asarray([mesh_vertex_normals[vertex_idx] for vertex_idx in vertices_idx])\n",
    "        # Estimate pom points normals\n",
    "        tree = geometry.KDTreeFlann(create_pcd(valid_vertices))\n",
    "        pom_normals = []\n",
    "        for point in pom_points:\n",
    "            [_, neighbors_idx, _] = tree.search_knn_vector_3d(point, num_neighbors)\n",
    "            normal = valid_vertex_normals[list(neighbors_idx)].mean(axis=0)\n",
    "            # Projection of normal onto section normal\n",
    "            normal = normal - section_normal*(np.dot(normal, section_normal)/np.dot(section_normal, section_normal)) \n",
    "            pom_normals.append(normal)\n",
    "        pom_normals = np.asarray(pom_normals)\n",
    "        pom_landmarks[key]['pom_points'] = pom_points\n",
    "        pom_landmarks[key]['pom_normals'] = pom_normals\n",
    "        # Base Pom Point Cloud\n",
    "        pcd_base = geometry.PointCloud()\n",
    "        pcd_base.points = utility.Vector3dVector(pom_points)\n",
    "        pcd_base.normals = utility.Vector3dVector(pom_normals)\n",
    "        base_poms.append(pcd_base)\n",
    "        \n",
    "        f.value += 1\n",
    "    return base_poms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolderdir = '/Users/nickf/Dropbox/Nicky T-Shirt Project/Subject Scans/Male Scans/Nicholas Ferrara/'\n",
    "meshfolderdir = '/Users/nickf/Dropbox/Nicky T-Shirt Project/Subject Scans/Male Scans/Nicholas Ferrara/NF_SCAN_V3/'\n",
    "meshdir = f'{meshfolderdir}NF_REGULAR_V1.4.1_M2_ADJ.ply'\n",
    "landmarksdir = f'{datafolderdir}NF_LANDMARKS.json'\n",
    "pomsdir = f'{datafolderdir}NF_POMS_V2.json'\n",
    "mesh, pom_landmarks = read_scan(meshdir, landmarksdir, pomsdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0383dc8798d435bb86c09bd200ba8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=1.0, max=23.0, min=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wireframe = create_wireframe(mesh, pom_landmarks, num_std=3, num_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.draw_geometries(wireframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_wireframe = []\n",
    "delta = 30\n",
    "for key in pom_landmarks:\n",
    "    transformed_wireframe.append(transform_wireframe(pom_landmarks, key, delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.draw_geometries([mesh]+wireframe+transformed_wireframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
